{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f51f19c",
   "metadata": {},
   "source": [
    "## Image Processing\n",
    "\n",
    "The code organizes the dataset of images into training and test sets for three categories: human, dalle, and stable. Random samples are selected from each category for training and the remaining samples are used for testing. The images are preprocessed, resized, and converted into numpy arrays. Training and test batches are created and saved as pickle files for later use."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T10:19:23.115125Z",
     "start_time": "2025-05-02T10:16:48.776048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tiền xử lý ảnh\n",
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    return np.array(img.convert(\"RGB\")) / 255.\n",
    "\n",
    "# Tạo dữ liệu kèm nhãn\n",
    "def create_data_with_labels(file_paths, folder, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file in file_paths:\n",
    "        if os.path.splitext(file)[1].lower() in ['.jpg', '.jpeg', '.png', '.webp']:\n",
    "            try:\n",
    "                img = preprocess_image(os.path.join(folder, file))\n",
    "                data.append(img)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "    return np.array(data), np.array(labels, dtype=np.float32)\n",
    "\n",
    "# Lấy mẫu ngẫu nhiên\n",
    "def get_random_samples(file_list, sample_size):\n",
    "    return random.sample(file_list, sample_size)\n",
    "\n",
    "# Phần còn lại sau khi lấy mẫu\n",
    "def get_remaining_samples(file_list, selected_samples):\n",
    "    return [sample for sample in file_list if sample not in selected_samples]\n",
    "\n",
    "# Phân loại AI theo độ khó\n",
    "def split_ai_by_difficulty(ai_files):\n",
    "    easy = [f for f in ai_files if f.lower().startswith(\"easy\")]\n",
    "    mid  = [f for f in ai_files if f.lower().startswith(\"mid\")]\n",
    "    hard = [f for f in ai_files if f.lower().startswith(\"hard\")]\n",
    "    return {'easy': easy, 'mid': mid, 'hard': hard}\n",
    "\n",
    "# Lưu file pickle\n",
    "def save_pickle_file(data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "# Hàm chính tổ chức dữ liệu\n",
    "def organize_data(human_path, ai_path, folder, human_train_size=700, ai_train_size=640):\n",
    "    os.makedirs(folder + 'file_names/', exist_ok=True)\n",
    "    os.makedirs(folder + 'train_batches/', exist_ok=True)\n",
    "    os.makedirs(folder + 'test_batches/', exist_ok=True)\n",
    "\n",
    "    human_files = os.listdir(human_path)\n",
    "    ai_files = os.listdir(ai_path)\n",
    "\n",
    "    easy_human_train = get_random_samples(human_files, human_train_size)\n",
    "    mid_human_train = get_random_samples(human_files, human_train_size)\n",
    "    hard_human_train = get_random_samples(human_files, human_train_size)\n",
    "    used_human = easy_human_train + mid_human_train + hard_human_train\n",
    "    human_test = get_remaining_samples(human_files, used_human)\n",
    "\n",
    "    ai_train = get_random_samples(ai_files, ai_train_size)\n",
    "    ai_test = get_remaining_samples(ai_files, ai_train)\n",
    "\n",
    "    save_pickle_file([easy_human_train + mid_human_train + hard_human_train, ai_train], folder + 'file_names/train.pickle')\n",
    "    save_pickle_file([human_test, ai_test], folder + 'file_names/test.pickle')\n",
    "\n",
    "    ai_by_difficulty = split_ai_by_difficulty(ai_files)\n",
    "    train_ratio = 0.7\n",
    "\n",
    "    def split_train_test(files):\n",
    "        train_size = int(len(files) * train_ratio)\n",
    "        train_files = get_random_samples(files, train_size)\n",
    "        test_files = get_remaining_samples(files, train_files)\n",
    "        return train_files, test_files\n",
    "\n",
    "    easy_ai_train, easy_ai_test = split_train_test(ai_by_difficulty['easy'])\n",
    "    mid_ai_train, mid_ai_test = split_train_test(ai_by_difficulty['mid'])\n",
    "    hard_ai_train, hard_ai_test = split_train_test(ai_by_difficulty['hard'])\n",
    "\n",
    "    save_pickle_file([easy_human_train, easy_ai_train], folder + 'file_names/train_easy.pickle')\n",
    "    save_pickle_file([mid_human_train, mid_ai_train], folder + 'file_names/train_mid.pickle')\n",
    "    save_pickle_file([hard_human_train, hard_ai_train], folder + 'file_names/train_hard.pickle')\n",
    "\n",
    "    save_pickle_file([human_test, easy_ai_test], folder + 'file_names/test_easy.pickle')\n",
    "    save_pickle_file([human_test, mid_ai_test], folder + 'file_names/test_mid.pickle')\n",
    "    save_pickle_file([human_test, hard_ai_test], folder + 'file_names/test_hard.pickle')\n",
    "\n",
    "    num_batches = 10\n",
    "    batch_size = 32\n",
    "    human_label = [0, 1]\n",
    "    ai_label = [1, 0]\n",
    "\n",
    "    for difficulty in ['easy', 'mid', 'hard']:\n",
    "        ai_train_batch = eval(f'{difficulty}_ai_train')\n",
    "        human_train_batch_all = eval(f'{difficulty}_human_train')\n",
    "\n",
    "        for batch in tqdm(range(num_batches), desc=f\"Processing {difficulty} train batches\"):\n",
    "            human_batch_files = human_train_batch_all[batch * batch_size:(batch + 1) * batch_size]\n",
    "            ai_batch_files = ai_train_batch[batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "            human_data, human_labels = create_data_with_labels(human_batch_files, human_path, human_label)\n",
    "            ai_data, ai_labels = create_data_with_labels(ai_batch_files, ai_path, ai_label)\n",
    "\n",
    "            if human_data.size == 0 or ai_data.size == 0:\n",
    "                print(f\"⚠️ Skipping empty batch {batch} for {difficulty}\")\n",
    "                continue\n",
    "\n",
    "            data_batch = {\n",
    "                'data': np.concatenate((human_data, ai_data), axis=0),\n",
    "                'labels': np.concatenate((human_labels, ai_labels), axis=0)\n",
    "            }\n",
    "\n",
    "            save_pickle_file(data_batch, f\"{folder}train_batches/batch_{difficulty}_{batch}.pickle\")\n",
    "\n",
    "    for difficulty in ['easy', 'mid', 'hard']:\n",
    "        ai_test_batch = eval(f'{difficulty}_ai_test')\n",
    "\n",
    "        human_test_data, human_test_labels = create_data_with_labels(human_test, human_path, human_label)\n",
    "        ai_test_data, ai_test_labels = create_data_with_labels(ai_test_batch, ai_path, ai_label)\n",
    "\n",
    "        if human_test_data.size == 0 or ai_test_data.size == 0:\n",
    "            print(f\"⚠️ Skipping empty test batch for {difficulty}\")\n",
    "            continue\n",
    "\n",
    "        test_data = np.concatenate((human_test_data, ai_test_data), axis=0)\n",
    "        test_labels = np.concatenate((human_test_labels, ai_test_labels), axis=0)\n",
    "        test_batch = {'data': test_data, 'labels': test_labels}\n",
    "\n",
    "        save_pickle_file(test_batch, f\"{folder}test_batches/test_{difficulty}_batch.pickle\")\n",
    "        print(f\"✅ Saved {difficulty} test batch\")\n",
    "\n",
    "# Gọi hàm\n",
    "organize_data(\n",
    "    human_path=\"train_data/real_and_fake_face/training_real\",\n",
    "    ai_path=\"train_data/real_and_fake_face/training_fake\",\n",
    "    folder=\"train_data/proccessed\"\n",
    ")\n"
   ],
   "id": "fbff5cd9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy train batches:  70%|███████   | 7/10 [00:47<00:12,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 6 for easy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy train batches:  80%|████████  | 8/10 [00:48<00:06,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 7 for easy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy train batches:  90%|█████████ | 9/10 [00:48<00:02,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 8 for easy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy train batches: 100%|██████████| 10/10 [00:49<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 9 for easy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing mid train batches: 100%|██████████| 10/10 [00:47<00:00,  4.75s/it]\n",
      "Processing hard train batches:  70%|███████   | 7/10 [00:23<00:07,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 6 for hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hard train batches:  80%|████████  | 8/10 [00:23<00:03,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 7 for hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hard train batches:  90%|█████████ | 9/10 [00:24<00:01,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 8 for hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hard train batches: 100%|██████████| 10/10 [00:24<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping empty batch 9 for hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved easy test batch\n",
      "✅ Saved mid test batch\n",
      "✅ Saved hard test batch\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "232f6045a5c8dcdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fe97613e7378c577"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "191611fd384d9acb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b3dc90ccfe2159f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
