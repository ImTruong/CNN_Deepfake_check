{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f51f19c",
   "metadata": {},
   "source": [
    "## Image Processing\n",
    "\n",
    "The code organizes the dataset of images into training and test sets for three categories: human, dalle, and stable. Random samples are selected from each category for training and the remaining samples are used for testing. The images are preprocessed, resized, and converted into numpy arrays. Training and test batches are created and saved as pickle files for later use."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T14:59:22.197865Z",
     "start_time": "2025-05-18T14:57:38.058792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tiền xử lý ảnh\n",
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    return np.array(img.convert(\"RGB\")) / 255.\n",
    "\n",
    "# Tạo dữ liệu kèm nhãn\n",
    "def create_data_with_labels(file_paths, folder, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file in file_paths:\n",
    "        if os.path.splitext(file)[1].lower() in ['.jpg', '.jpeg', '.png', '.webp']:\n",
    "            try:\n",
    "                img = preprocess_image(os.path.join(folder, file))\n",
    "                data.append(img)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "    return np.array(data), np.array(labels, dtype=np.float32)\n",
    "\n",
    "# Phân loại AI theo độ khó\n",
    "def split_ai_by_difficulty(ai_files):\n",
    "    easy = [f for f in ai_files if f.lower().startswith(\"easy\")]\n",
    "    mid  = [f for f in ai_files if f.lower().startswith(\"mid\")]\n",
    "    hard = [f for f in ai_files if f.lower().startswith(\"hard\")]\n",
    "    return {'easy': easy, 'mid': mid, 'hard': hard}\n",
    "\n",
    "# Lưu file pickle\n",
    "def save_pickle_file(data, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "# Hàm chính tổ chức dữ liệu\n",
    "def organize_data(human_path, ai_path, folder):\n",
    "    os.makedirs(folder + 'file_names/', exist_ok=True)\n",
    "    os.makedirs(folder + 'train_batches/', exist_ok=True)\n",
    "    os.makedirs(folder + 'val_batches/', exist_ok=True)\n",
    "    os.makedirs(folder + 'test_batches/', exist_ok=True)\n",
    "\n",
    "    human_files = os.listdir(human_path)\n",
    "    ai_files = os.listdir(ai_path)\n",
    "\n",
    "    print(f\"Total human files: {len(human_files)}\")\n",
    "    print(f\"Total AI files: {len(ai_files)}\")\n",
    "\n",
    "    # Tỉ lệ chia\n",
    "    train_ratio = 0.6\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "\n",
    "    # Phân loại AI theo độ khó\n",
    "    ai_by_difficulty = split_ai_by_difficulty(ai_files)\n",
    "    for diff, files in ai_by_difficulty.items():\n",
    "        print(f\"{diff.capitalize()} AI files: {len(files)}\")\n",
    "\n",
    "    def split_by_ratio(files):\n",
    "        random.shuffle(files)\n",
    "        train_size = int(len(files) * train_ratio)\n",
    "        val_size = int(len(files) * val_ratio)\n",
    "\n",
    "        train_files = files[:train_size]\n",
    "        val_files = files[train_size:train_size+val_size]\n",
    "        test_files = files[train_size+val_size:]\n",
    "\n",
    "        return train_files, val_files, test_files\n",
    "\n",
    "    # Phân chia human theo độ khó (vẫn giữ tỷ lệ giống nhau)\n",
    "    easy_human_train, easy_human_val, easy_human_test = split_by_ratio(human_files)\n",
    "    mid_human_train, mid_human_val, mid_human_test = split_by_ratio(human_files)\n",
    "    hard_human_train, hard_human_val, hard_human_test = split_by_ratio(human_files)\n",
    "\n",
    "    # Phân chia AI theo độ khó\n",
    "    easy_ai_train, easy_ai_val, easy_ai_test = split_by_ratio(ai_by_difficulty['easy'])\n",
    "    mid_ai_train, mid_ai_val, mid_ai_test = split_by_ratio(ai_by_difficulty['mid'])\n",
    "    hard_ai_train, hard_ai_val, hard_ai_test = split_by_ratio(ai_by_difficulty['hard'])\n",
    "\n",
    "    # Lưu danh sách file theo độ khó\n",
    "    save_pickle_file([easy_human_train, easy_ai_train], folder + 'file_names/train_easy.pickle')\n",
    "    save_pickle_file([mid_human_train, mid_ai_train], folder + 'file_names/train_mid.pickle')\n",
    "    save_pickle_file([hard_human_train, hard_ai_train], folder + 'file_names/train_hard.pickle')\n",
    "\n",
    "    save_pickle_file([easy_human_val, easy_ai_val], folder + 'file_names/val_easy.pickle')\n",
    "    save_pickle_file([mid_human_val, mid_ai_val], folder + 'file_names/val_mid.pickle')\n",
    "    save_pickle_file([hard_human_val, hard_ai_val], folder + 'file_names/val_hard.pickle')\n",
    "\n",
    "    save_pickle_file([easy_human_test, easy_ai_test], folder + 'file_names/test_easy.pickle')\n",
    "    save_pickle_file([mid_human_test, mid_ai_test], folder + 'file_names/test_mid.pickle')\n",
    "    save_pickle_file([hard_human_test, hard_ai_test], folder + 'file_names/test_hard.pickle')\n",
    "\n",
    "    # Tạo batch dữ liệu\n",
    "    num_batches = 10\n",
    "    batch_size = 32\n",
    "    human_label = [0, 1]  # [real, fake]\n",
    "    ai_label = [1, 0]     # [fake, real]\n",
    "\n",
    "    # Tạo batch training\n",
    "    for difficulty in ['easy', 'mid', 'hard']:\n",
    "        ai_train_files = eval(f'{difficulty}_ai_train')\n",
    "        human_train_files = eval(f'{difficulty}_human_train')\n",
    "\n",
    "        # Đảm bảo đủ số lượng batch\n",
    "        max_batch = min(len(ai_train_files) // batch_size, len(human_train_files) // batch_size, num_batches)\n",
    "\n",
    "        for batch in tqdm(range(max_batch), desc=f\"Processing {difficulty} train batches\"):\n",
    "            human_batch_files = human_train_files[batch * batch_size:(batch + 1) * batch_size]\n",
    "            ai_batch_files = ai_train_files[batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "            human_data, human_labels = create_data_with_labels(human_batch_files, human_path, human_label)\n",
    "            ai_data, ai_labels = create_data_with_labels(ai_batch_files, ai_path, ai_label)\n",
    "\n",
    "            if human_data.size == 0 or ai_data.size == 0:\n",
    "                print(f\"⚠️ Skipping empty batch {batch} for {difficulty}\")\n",
    "                continue\n",
    "\n",
    "            data_batch = {\n",
    "                'data': np.concatenate((human_data, ai_data), axis=0),\n",
    "                'labels': np.concatenate((human_labels, ai_labels), axis=0)\n",
    "            }\n",
    "\n",
    "            save_pickle_file(data_batch, f\"{folder}train_batches/batch_{difficulty}_{batch}.pickle\")\n",
    "\n",
    "    # Tạo batch validation\n",
    "    for difficulty in ['easy', 'mid', 'hard']:\n",
    "        ai_val_files = eval(f'{difficulty}_ai_val')\n",
    "        human_val_files = eval(f'{difficulty}_human_val')\n",
    "\n",
    "        val_batch_size = min(len(ai_val_files), len(human_val_files), batch_size*2)\n",
    "\n",
    "        if val_batch_size < batch_size:\n",
    "            print(f\"⚠️ Warning: Not enough validation data for {difficulty}. Using all available.\")\n",
    "\n",
    "        human_val_batch = human_val_files[:val_batch_size//2]\n",
    "        ai_val_batch = ai_val_files[:val_batch_size//2]\n",
    "\n",
    "        human_data, human_labels = create_data_with_labels(human_val_batch, human_path, human_label)\n",
    "        ai_data, ai_labels = create_data_with_labels(ai_val_batch, ai_path, ai_label)\n",
    "\n",
    "        if human_data.size == 0 or ai_data.size == 0:\n",
    "            print(f\"⚠️ Skipping empty validation batch for {difficulty}\")\n",
    "            continue\n",
    "\n",
    "        val_batch = {\n",
    "            'data': np.concatenate((human_data, ai_data), axis=0),\n",
    "            'labels': np.concatenate((human_labels, ai_labels), axis=0)\n",
    "        }\n",
    "\n",
    "        save_pickle_file(val_batch, f\"{folder}val_batches/val_{difficulty}_batch.pickle\")\n",
    "        print(f\"✅ Saved {difficulty} validation batch\")\n",
    "\n",
    "    # Tạo batch test\n",
    "    for difficulty in ['easy', 'mid', 'hard']:\n",
    "        ai_test_files = eval(f'{difficulty}_ai_test')\n",
    "        human_test_files = eval(f'{difficulty}_human_test')\n",
    "\n",
    "        test_batch_size = min(len(ai_test_files), len(human_test_files), batch_size*2)\n",
    "\n",
    "        if test_batch_size < batch_size:\n",
    "            print(f\"⚠️ Warning: Not enough test data for {difficulty}. Using all available.\")\n",
    "\n",
    "        human_test_batch = human_test_files[:test_batch_size//2]\n",
    "        ai_test_batch = ai_test_files[:test_batch_size//2]\n",
    "\n",
    "        human_data, human_labels = create_data_with_labels(human_test_batch, human_path, human_label)\n",
    "        ai_data, ai_labels = create_data_with_labels(ai_test_batch, ai_path, ai_label)\n",
    "\n",
    "        if human_data.size == 0 or ai_data.size == 0:\n",
    "            print(f\"⚠️ Skipping empty test batch for {difficulty}\")\n",
    "            continue\n",
    "\n",
    "        test_batch = {\n",
    "            'data': np.concatenate((human_data, ai_data), axis=0),\n",
    "            'labels': np.concatenate((human_labels, ai_labels), axis=0)\n",
    "        }\n",
    "\n",
    "        save_pickle_file(test_batch, f\"{folder}test_batches/test_{difficulty}_batch.pickle\")\n",
    "        print(f\"✅ Saved {difficulty} test batch\")\n",
    "\n",
    "# Gọi hàm\n",
    "organize_data(\n",
    "    human_path=\"train_data/real_and_fake_face/training_real\",\n",
    "    ai_path=\"train_data/real_and_fake_face/training_fake\",\n",
    "    folder=\"train_data/processed/\"\n",
    ")"
   ],
   "id": "fbff5cd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total human files: 1081\n",
      "Total AI files: 960\n",
      "Easy AI files: 240\n",
      "Mid AI files: 480\n",
      "Hard AI files: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy train batches: 100%|██████████| 4/4 [00:19<00:00,  4.78s/it]\n",
      "Processing mid train batches: 100%|██████████| 9/9 [00:42<00:00,  4.71s/it]\n",
      "Processing hard train batches: 100%|██████████| 4/4 [00:18<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved easy validation batch\n",
      "✅ Saved mid validation batch\n",
      "✅ Saved hard validation batch\n",
      "✅ Saved easy test batch\n",
      "✅ Saved mid test batch\n",
      "✅ Saved hard test batch\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "232f6045a5c8dcdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fe97613e7378c577"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "191611fd384d9acb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b3dc90ccfe2159f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
